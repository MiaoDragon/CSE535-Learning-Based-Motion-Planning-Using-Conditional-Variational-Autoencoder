{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('cse535': conda)",
   "display_name": "Python 3.8.5 64-bit ('cse535': conda)",
   "metadata": {
    "interpreter": {
     "hash": "426ee9227c26455faf607ec3a590f915894f1ba5a016c6956515b9ee914a0b72"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.cuda.set_device(3)\n"
   ]
  },
  {
   "source": [
    "# define CVAE model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, cond_size, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.mu_network = nn.Sequential(\n",
    "            nn.Linear(input_size+cond_size, 512), nn.PReLU(), nn.Dropout(),\n",
    "            nn.Linear(512, 256), nn.PReLU(), nn.Dropout(),\n",
    "            nn.Linear(256, 32), nn.PReLU(),\n",
    "            nn.Linear(32, latent_size)\n",
    "        )\n",
    "        # we set the covariance matrix to be diag([sigma_1,...,sigma_k])\n",
    "        self.log_sigma_pow2_network = nn.Sequential(\n",
    "            nn.Linear(input_size+cond_size, 512), nn.PReLU(), nn.Dropout(),\n",
    "            nn.Linear(512, 256), nn.PReLU(), nn.Dropout(),\n",
    "            nn.Linear(256, 32), nn.PReLU(),\n",
    "            nn.Linear(32, latent_size)\n",
    "        )\n",
    "        self.output_size = latent_size\n",
    "        self.latent_size = latent_size\n",
    "        self.input_size = input_size\n",
    "        self.cond_size = cond_size\n",
    "    def forward(self, x, y):\n",
    "        # input tensor shape: BxK1, BxK2\n",
    "        input = torch.cat([x,y], dim=1)\n",
    "        mu = self.mu_network(input)\n",
    "        log_sigma_pow2 = self.log_sigma_pow2_network(input)\n",
    "        return mu, log_sigma_pow2\n",
    "\n",
    "    def sample(self, mu, log_sigma_pow2, L):\n",
    "        # given the computed mu, and sigma, obtain L samples by reparameterization\n",
    "        # draw standard normal distribution\n",
    "        # input: Bxk\n",
    "        # return: LxBxk\n",
    "        eps = torch.randn((L,len(mu),self.latent_size))\n",
    "        if log_sigma_pow2.is_cuda:\n",
    "            eps = eps.cuda()\n",
    "        eps = eps * torch.exp(log_sigma_pow2/2)\n",
    "        eps = eps + mu\n",
    "        return eps\n",
    "\n",
    "    def kl_divergence(self, mu, log_sigma_pow2):\n",
    "        # given mu and log(sigma^2), obtain the KL divergence relative to N(0,I)\n",
    "        # using formula from https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes\n",
    "        # input: BxK\n",
    "        # output: B\n",
    "        res = 1.0 / 2 * (-torch.sum(log_sigma_pow2, dim=1)-self.output_size+\\\n",
    "                         torch.sum(torch.exp(log_sigma_pow2), dim=1)+torch.sum(mu*mu, dim=1))\n",
    "        return res\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, cond_size, output_size, sigma=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.mu_network = nn.Sequential(\n",
    "            nn.Linear(latent_size+cond_size, 512), nn.PReLU(), nn.Dropout(),\n",
    "            nn.Linear(512, 256), nn.PReLU(), nn.Dropout(),\n",
    "            nn.Linear(256, 32), nn.PReLU(),\n",
    "            nn.Linear(32, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.latent_size = latent_size\n",
    "        self.cond_size = cond_size\n",
    "        self.sigma = sigma\n",
    "        self.sigma_2 = sigma*sigma\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        input = torch.cat([z,y],dim=1)\n",
    "        mu = self.mu_network(input)\n",
    "        return mu\n",
    "\n",
    "    def sample(self, mu):\n",
    "        # use the computed mu to generate sample\n",
    "        # input: BxN\n",
    "        eps = torch.randn(mu.size())\n",
    "        if mu.is_cuda:\n",
    "            eps = eps.cuda()\n",
    "        eps = eps * self.sigma + mu\n",
    "        return eps\n",
    "\n",
    "    def generation_loss(self, x, mu):\n",
    "        # input:\n",
    "        # - mu: LxBxN\n",
    "        # - x: BxN\n",
    "        # output: BxN\n",
    "        # formula: 1/L * sum_z log(N(x; mu, sigma^2I))\n",
    "        #       => -1/L \\sum_z 1/2*(x-mu)^T(x-mu)/(sigma^2)\n",
    "\n",
    "        #res = - 1.0/2*(x-mu)*(x-mu)/self.sigma_2  # using normal distribution for p(x|z,y)\n",
    "        #print(x.repeat(len(mu),1,1).size())\n",
    "        #print(mu.size())\n",
    "        res = -torch.nn.functional.binary_cross_entropy(mu, x.repeat(len(mu),1,1), reduction='none')\n",
    "        #print(res.cpu())\n",
    "        #print(res.size())\n",
    "        res = torch.sum(res, dim=2) # sum up (x-mu)^2\n",
    "        # calculate the mean w.r.t. first dimension (L)\n",
    "        res = torch.mean(res, dim=0)\n",
    "        return res\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, cond_size):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_size, cond_size, latent_size)\n",
    "        self.decoder = Decoder(latent_size, cond_size, input_size, sigma=0.1)\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        self.cond_size = cond_size\n",
    "\n",
    "    def train_forward(self, x, y, L=10):\n",
    "        # get necessary signals from the input\n",
    "        z_mu, z_log_sigma_pow2 = self.encoder(x, y)\n",
    "        # generate samples of z using the mean and variance\n",
    "        z = self.encoder.sample(z_mu, z_log_sigma_pow2, L)\n",
    "        y_extended = y.repeat(len(z),1).view(-1,self.cond_size)\n",
    "        z = z.view(-1,self.latent_size)  # B and L together first\n",
    "        if x.is_cuda:\n",
    "            z.cuda()\n",
    "        # copy y so we have shape: LxBxc\n",
    "        x_mu = self.decoder(z, y_extended).view(L,-1,self.input_size)\n",
    "        return z_mu,z_log_sigma_pow2, z, x_mu\n",
    "\n",
    "    def gen_forward(self, y):\n",
    "        # randomly sample a latent z\n",
    "        z = torch.randn(len(y),self.latent_size)\n",
    "        if y.is_cuda:\n",
    "            z = z.cuda()\n",
    "        x_mean = self.decoder(z, y)\n",
    "        x = self.decoder.sample(x_mean)\n",
    "        return x\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit testing for encoder\n",
    "# test sample\n",
    "\n",
    "# def sample(self, mu, log_sigma_pow2, L):\n",
    "#     # given the computed mu, and sigma, obtain L samples by reparameterization\n",
    "#     # draw standard normal distribution\n",
    "#     # input: Bxk\n",
    "#     # return: LxBxk\n",
    "input_size = 20\n",
    "cond_size = 2\n",
    "latent_size = 5\n",
    "encoder = Encoder(input_size, cond_size, latent_size)\n",
    "mu = torch.ones((10,5))\n",
    "sigma = torch.abs(torch.randn(10,5))\n",
    "log_sigma_pow2 = torch.log(sigma*sigma)\n",
    "L = 1000\n",
    "z = encoder.sample(mu, log_sigma_pow2, L)\n",
    "\n",
    "\n",
    "# test KL divergence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(z)\n",
    "unit_testing = False\n",
    "if unit_testing:\n",
    "    print('estimated mean: ')\n",
    "    print(torch.mean(z, dim=0))\n",
    "    print('estimated std: ')\n",
    "    print(torch.std(z, dim=0))\n",
    "    print('true mean:')\n",
    "    print(mu)\n",
    "    print('true std:')\n",
    "    print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "def idx2onehot(idx, n):\n",
    "\n",
    "    assert torch.max(idx).item() < n\n",
    "    if idx.dim() == 1:\n",
    "        idx = idx.unsqueeze(1)\n",
    "\n",
    "    onehot = torch.zeros(idx.size(0), n)\n",
    "    onehot.scatter_(1, idx, 1)\n",
    "\n",
    "    return onehot\n",
    "\n",
    "def main():\n",
    "    batch_size = 32\n",
    "    input_size = 28*28\n",
    "    latent_size = 16\n",
    "    cond_size = 10\n",
    "    learning_rate = 0.001\n",
    "    num_epoch = 20\n",
    "    print_every = 100\n",
    "\n",
    "    #device = torch.device('cuda:0')\n",
    "\n",
    "    dataset = MNIST(\n",
    "            root='data', train=True, transform=transforms.ToTensor(),\n",
    "            download=True)\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    cvae = CVAE(input_size, latent_size, cond_size)\n",
    "    cvae.cuda()\n",
    "    optimizer = torch.optim.Adam(cvae.parameters(), learning_rate)\n",
    "\n",
    "    logs = defaultdict(list)\n",
    "    print('start training...')\n",
    "    for epoch in range(num_epoch):\n",
    "        for iter, (x, y) in enumerate(data_loader):\n",
    "            y = idx2onehot(y, n=10)\n",
    "            x = x.view(-1,28*28)\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            #x, y = x.to(device), y.to(device)\n",
    "            z_mu,z_log_sigma_pow2, z, x_mu = cvae.train_forward(x, y, L=10)\n",
    "            kl_divergence = cvae.encoder.kl_divergence(z_mu, z_log_sigma_pow2)\n",
    "            generation_loss = cvae.decoder.generation_loss(x, x_mu)\n",
    "            loss_i = -generation_loss + kl_divergence\n",
    "            loss_i = torch.mean(loss_i)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_i.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            logs['loss'].append(loss_i.item())\n",
    "            if iter % print_every == 0:\n",
    "                print('epoch: %d, batch: %d, loss: %f' % (epoch, iter, loss_i.item()))\n",
    "                # save the reconstructed inference\n",
    "\n",
    "                y = torch.arange(0, 10).long().unsqueeze(1)\n",
    "                y_onehot = idx2onehot(y, n=10)\n",
    "                y_onehot = y_onehot.cuda()\n",
    "                x = cvae.gen_forward(y_onehot)\n",
    "\n",
    "                plt.figure()\n",
    "                plt.figure(figsize=(5, 10))\n",
    "                for p in range(10):\n",
    "                    plt.subplot(5, 2, p+1)\n",
    "                    plt.text(\n",
    "                        0, 0, \"y={:d}\".format(y[p].item()), color='black',\n",
    "                        backgroundcolor='white', fontsize=8)\n",
    "                    plt.imshow(x[p].cpu().view(28, 28).data.numpy())\n",
    "                    plt.axis('off')\n",
    "                fig_root = 'plots'\n",
    "                os.makedirs(os.path.join(fig_root), exist_ok=True)\n",
    "                plt.savefig(\n",
    "                    os.path.join(fig_root,\n",
    "                                 \"epoch{:d}batch{:d}.png\".format(epoch, iter)),\n",
    "                    dpi=300)\n",
    "                plt.clf()\n",
    "                plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start training...\n",
      "epoch: 0, batch: 0, loss: 545.728516\n",
      "epoch: 0, batch: 100, loss: 192.460876\n",
      "epoch: 0, batch: 200, loss: 206.504166\n",
      "epoch: 0, batch: 300, loss: 189.780090\n",
      "epoch: 0, batch: 400, loss: 183.643265\n",
      "epoch: 0, batch: 500, loss: 188.637756\n",
      "epoch: 0, batch: 600, loss: 159.811295\n",
      "epoch: 0, batch: 700, loss: 191.310120\n",
      "epoch: 0, batch: 800, loss: 163.205643\n",
      "epoch: 0, batch: 900, loss: 180.235931\n",
      "epoch: 0, batch: 1000, loss: 172.559143\n",
      "epoch: 0, batch: 1100, loss: 177.218475\n",
      "epoch: 0, batch: 1200, loss: 162.661575\n",
      "epoch: 0, batch: 1300, loss: 180.538254\n",
      "epoch: 0, batch: 1400, loss: 163.075226\n",
      "epoch: 0, batch: 1500, loss: 147.700836\n",
      "epoch: 0, batch: 1600, loss: 150.922333\n",
      "epoch: 0, batch: 1700, loss: 155.591614\n",
      "epoch: 0, batch: 1800, loss: 159.610641\n",
      "epoch: 1, batch: 0, loss: 152.263580\n",
      "epoch: 1, batch: 100, loss: 146.241974\n",
      "epoch: 1, batch: 200, loss: 141.831497\n",
      "epoch: 1, batch: 300, loss: 137.241791\n",
      "epoch: 1, batch: 400, loss: 138.629990\n",
      "epoch: 1, batch: 500, loss: 152.744217\n",
      "epoch: 1, batch: 600, loss: 150.104340\n",
      "epoch: 1, batch: 700, loss: 137.277740\n",
      "epoch: 1, batch: 800, loss: 151.562134\n",
      "epoch: 1, batch: 900, loss: 159.038910\n",
      "epoch: 1, batch: 1000, loss: 149.414062\n",
      "epoch: 1, batch: 1100, loss: 158.329636\n",
      "epoch: 1, batch: 1200, loss: 143.168671\n",
      "epoch: 1, batch: 1300, loss: 149.957916\n",
      "epoch: 1, batch: 1400, loss: 145.640076\n",
      "epoch: 1, batch: 1500, loss: 136.710297\n",
      "epoch: 1, batch: 1600, loss: 148.076736\n",
      "epoch: 1, batch: 1700, loss: 151.308899\n",
      "epoch: 1, batch: 1800, loss: 151.204285\n",
      "epoch: 2, batch: 0, loss: 154.961700\n",
      "epoch: 2, batch: 100, loss: 137.768570\n",
      "epoch: 2, batch: 200, loss: 150.220078\n",
      "epoch: 2, batch: 300, loss: 137.159027\n",
      "epoch: 2, batch: 400, loss: 130.590393\n",
      "epoch: 2, batch: 500, loss: 145.162292\n",
      "epoch: 2, batch: 600, loss: 147.448303\n",
      "epoch: 2, batch: 700, loss: 135.774979\n",
      "epoch: 2, batch: 800, loss: 152.966080\n",
      "epoch: 2, batch: 900, loss: 139.719360\n",
      "epoch: 2, batch: 1000, loss: 142.990631\n",
      "epoch: 2, batch: 1100, loss: 145.209717\n",
      "epoch: 2, batch: 1200, loss: 151.993362\n",
      "epoch: 2, batch: 1300, loss: 142.201538\n",
      "epoch: 2, batch: 1400, loss: 129.920792\n",
      "epoch: 2, batch: 1500, loss: 134.178650\n",
      "epoch: 2, batch: 1600, loss: 140.094604\n",
      "epoch: 2, batch: 1700, loss: 130.089188\n",
      "epoch: 2, batch: 1800, loss: 134.248062\n",
      "epoch: 3, batch: 0, loss: 134.465393\n",
      "epoch: 3, batch: 100, loss: 156.128510\n",
      "epoch: 3, batch: 200, loss: 138.425674\n",
      "epoch: 3, batch: 300, loss: 140.729385\n",
      "epoch: 3, batch: 400, loss: 147.310196\n",
      "epoch: 3, batch: 500, loss: 144.610153\n",
      "epoch: 3, batch: 600, loss: 139.654785\n",
      "epoch: 3, batch: 700, loss: 135.317566\n",
      "epoch: 3, batch: 800, loss: 138.225067\n",
      "epoch: 3, batch: 900, loss: 136.892532\n",
      "epoch: 3, batch: 1000, loss: 140.283020\n",
      "epoch: 3, batch: 1100, loss: 138.610626\n",
      "epoch: 3, batch: 1200, loss: 122.470863\n",
      "epoch: 3, batch: 1300, loss: 144.050980\n",
      "epoch: 3, batch: 1400, loss: 132.334061\n",
      "epoch: 3, batch: 1500, loss: 151.257263\n",
      "epoch: 3, batch: 1600, loss: 144.106339\n",
      "epoch: 3, batch: 1700, loss: 149.911682\n",
      "epoch: 3, batch: 1800, loss: 137.861481\n",
      "epoch: 4, batch: 0, loss: 132.380493\n",
      "epoch: 4, batch: 100, loss: 133.614349\n",
      "epoch: 4, batch: 200, loss: 153.182266\n",
      "epoch: 4, batch: 300, loss: 134.256317\n",
      "epoch: 4, batch: 400, loss: 147.849731\n",
      "epoch: 4, batch: 500, loss: 142.821457\n",
      "epoch: 4, batch: 600, loss: 138.869705\n",
      "epoch: 4, batch: 700, loss: 144.622681\n",
      "epoch: 4, batch: 800, loss: 144.280823\n",
      "epoch: 4, batch: 900, loss: 145.559631\n",
      "epoch: 4, batch: 1000, loss: 141.476379\n",
      "epoch: 4, batch: 1100, loss: 141.343338\n",
      "epoch: 4, batch: 1200, loss: 137.892426\n",
      "epoch: 4, batch: 1300, loss: 134.770935\n",
      "epoch: 4, batch: 1400, loss: 125.997780\n",
      "epoch: 4, batch: 1500, loss: 125.931900\n",
      "epoch: 4, batch: 1600, loss: 141.661407\n",
      "epoch: 4, batch: 1700, loss: 138.903320\n",
      "epoch: 4, batch: 1800, loss: 144.229721\n",
      "epoch: 5, batch: 0, loss: 126.911423\n",
      "epoch: 5, batch: 100, loss: 135.846725\n",
      "epoch: 5, batch: 200, loss: 131.907333\n",
      "epoch: 5, batch: 300, loss: 152.419968\n",
      "epoch: 5, batch: 400, loss: 148.148300\n",
      "epoch: 5, batch: 500, loss: 152.894257\n",
      "epoch: 5, batch: 600, loss: 151.014725\n",
      "epoch: 5, batch: 700, loss: 132.556183\n",
      "epoch: 5, batch: 800, loss: 132.695953\n",
      "epoch: 5, batch: 900, loss: 142.820816\n",
      "epoch: 5, batch: 1000, loss: 134.380051\n",
      "epoch: 5, batch: 1100, loss: 139.379944\n",
      "epoch: 5, batch: 1200, loss: 140.682831\n",
      "epoch: 5, batch: 1300, loss: 135.739655\n",
      "epoch: 5, batch: 1400, loss: 136.795700\n",
      "epoch: 5, batch: 1500, loss: 144.292328\n",
      "epoch: 5, batch: 1600, loss: 136.183914\n",
      "epoch: 5, batch: 1700, loss: 142.424393\n",
      "epoch: 5, batch: 1800, loss: 140.024521\n",
      "epoch: 6, batch: 0, loss: 138.931641\n",
      "epoch: 6, batch: 100, loss: 137.703033\n",
      "epoch: 6, batch: 200, loss: 131.041931\n",
      "epoch: 6, batch: 300, loss: 131.507675\n",
      "epoch: 6, batch: 400, loss: 148.689957\n",
      "epoch: 6, batch: 500, loss: 141.399307\n",
      "epoch: 6, batch: 600, loss: 131.665451\n",
      "epoch: 6, batch: 700, loss: 135.098785\n",
      "epoch: 6, batch: 800, loss: 139.471863\n",
      "epoch: 6, batch: 900, loss: 131.875687\n",
      "epoch: 6, batch: 1000, loss: 136.017487\n",
      "epoch: 6, batch: 1100, loss: 150.504974\n",
      "epoch: 6, batch: 1200, loss: 148.567963\n",
      "epoch: 6, batch: 1300, loss: 118.210312\n",
      "epoch: 6, batch: 1400, loss: 138.937668\n",
      "epoch: 6, batch: 1500, loss: 138.868469\n",
      "epoch: 6, batch: 1600, loss: 133.194794\n",
      "epoch: 6, batch: 1700, loss: 143.100540\n",
      "epoch: 6, batch: 1800, loss: 148.185699\n",
      "epoch: 7, batch: 0, loss: 129.701691\n",
      "epoch: 7, batch: 100, loss: 142.127686\n",
      "epoch: 7, batch: 200, loss: 140.039078\n",
      "epoch: 7, batch: 300, loss: 149.650360\n",
      "epoch: 7, batch: 400, loss: 133.021851\n",
      "epoch: 7, batch: 500, loss: 142.932800\n",
      "epoch: 7, batch: 600, loss: 116.682442\n",
      "epoch: 7, batch: 700, loss: 136.176697\n",
      "epoch: 7, batch: 800, loss: 143.098221\n",
      "epoch: 7, batch: 900, loss: 127.375137\n",
      "epoch: 7, batch: 1000, loss: 155.293747\n",
      "epoch: 7, batch: 1100, loss: 138.156494\n",
      "epoch: 7, batch: 1200, loss: 138.673248\n",
      "epoch: 7, batch: 1300, loss: 122.741791\n",
      "epoch: 7, batch: 1400, loss: 134.834595\n",
      "epoch: 7, batch: 1500, loss: 131.704681\n",
      "epoch: 7, batch: 1600, loss: 136.878265\n",
      "epoch: 7, batch: 1700, loss: 129.840179\n",
      "epoch: 7, batch: 1800, loss: 138.863251\n",
      "epoch: 8, batch: 0, loss: 137.202881\n",
      "epoch: 8, batch: 100, loss: 130.075439\n",
      "epoch: 8, batch: 200, loss: 128.424622\n",
      "epoch: 8, batch: 300, loss: 148.751312\n",
      "epoch: 8, batch: 400, loss: 122.572029\n",
      "epoch: 8, batch: 500, loss: 144.979523\n",
      "epoch: 8, batch: 600, loss: 141.073547\n",
      "epoch: 8, batch: 700, loss: 140.378052\n",
      "epoch: 8, batch: 800, loss: 137.817078\n",
      "epoch: 8, batch: 900, loss: 143.438644\n",
      "epoch: 8, batch: 1000, loss: 146.332001\n",
      "epoch: 8, batch: 1100, loss: 134.864273\n",
      "epoch: 8, batch: 1200, loss: 133.381378\n",
      "epoch: 8, batch: 1300, loss: 148.273529\n",
      "epoch: 8, batch: 1400, loss: 131.074036\n",
      "epoch: 8, batch: 1500, loss: 130.831787\n",
      "epoch: 8, batch: 1600, loss: 133.011963\n",
      "epoch: 8, batch: 1700, loss: 149.597870\n",
      "epoch: 8, batch: 1800, loss: 137.315887\n",
      "epoch: 9, batch: 0, loss: 138.782211\n",
      "epoch: 9, batch: 100, loss: 148.877716\n",
      "epoch: 9, batch: 200, loss: 119.228706\n",
      "epoch: 9, batch: 300, loss: 131.404831\n",
      "epoch: 9, batch: 400, loss: 150.432846\n",
      "epoch: 9, batch: 500, loss: 131.770691\n",
      "epoch: 9, batch: 600, loss: 137.458939\n",
      "epoch: 9, batch: 700, loss: 139.709381\n",
      "epoch: 9, batch: 800, loss: 137.543472\n",
      "epoch: 9, batch: 900, loss: 146.349030\n",
      "epoch: 9, batch: 1000, loss: 139.127411\n",
      "epoch: 9, batch: 1100, loss: 141.144012\n",
      "epoch: 9, batch: 1200, loss: 133.974991\n",
      "epoch: 9, batch: 1300, loss: 148.577698\n",
      "epoch: 9, batch: 1400, loss: 140.327103\n",
      "epoch: 9, batch: 1500, loss: 139.081451\n",
      "epoch: 9, batch: 1600, loss: 147.316986\n",
      "epoch: 9, batch: 1700, loss: 145.621597\n",
      "epoch: 9, batch: 1800, loss: 140.029694\n",
      "epoch: 10, batch: 0, loss: 137.435242\n",
      "epoch: 10, batch: 100, loss: 134.337952\n",
      "epoch: 10, batch: 200, loss: 143.672195\n",
      "epoch: 10, batch: 300, loss: 144.501709\n",
      "epoch: 10, batch: 400, loss: 141.923141\n",
      "epoch: 10, batch: 500, loss: 124.360779\n",
      "epoch: 10, batch: 600, loss: 134.057709\n",
      "epoch: 10, batch: 700, loss: 116.353500\n",
      "epoch: 10, batch: 800, loss: 126.612511\n",
      "epoch: 10, batch: 900, loss: 127.834457\n",
      "epoch: 10, batch: 1000, loss: 141.889816\n",
      "epoch: 10, batch: 1100, loss: 118.677849\n",
      "epoch: 10, batch: 1200, loss: 147.149963\n",
      "epoch: 10, batch: 1300, loss: 136.727020\n",
      "epoch: 10, batch: 1400, loss: 135.182526\n",
      "epoch: 10, batch: 1500, loss: 136.025131\n",
      "epoch: 10, batch: 1600, loss: 138.797913\n",
      "epoch: 10, batch: 1700, loss: 138.772232\n",
      "epoch: 10, batch: 1800, loss: 148.880829\n",
      "epoch: 11, batch: 0, loss: 149.720520\n",
      "epoch: 11, batch: 100, loss: 136.756317\n",
      "epoch: 11, batch: 200, loss: 127.248764\n",
      "epoch: 11, batch: 300, loss: 140.112915\n",
      "epoch: 11, batch: 400, loss: 144.811829\n",
      "epoch: 11, batch: 500, loss: 140.605194\n",
      "epoch: 11, batch: 600, loss: 123.697594\n",
      "epoch: 11, batch: 700, loss: 144.003510\n",
      "epoch: 11, batch: 800, loss: 133.356369\n",
      "epoch: 11, batch: 900, loss: 132.852524\n",
      "epoch: 11, batch: 1000, loss: 137.008026\n",
      "epoch: 11, batch: 1100, loss: 132.274445\n",
      "epoch: 11, batch: 1200, loss: 140.824036\n",
      "epoch: 11, batch: 1300, loss: 132.560104\n",
      "epoch: 11, batch: 1400, loss: 134.623276\n",
      "epoch: 11, batch: 1500, loss: 137.836929\n",
      "epoch: 11, batch: 1600, loss: 134.411331\n",
      "epoch: 11, batch: 1700, loss: 137.954376\n",
      "epoch: 11, batch: 1800, loss: 142.752747\n",
      "epoch: 12, batch: 0, loss: 131.173279\n",
      "epoch: 12, batch: 100, loss: 132.813049\n",
      "epoch: 12, batch: 200, loss: 149.608444\n",
      "epoch: 12, batch: 300, loss: 147.317429\n",
      "epoch: 12, batch: 400, loss: 138.654312\n",
      "epoch: 12, batch: 500, loss: 147.273712\n",
      "epoch: 12, batch: 600, loss: 138.882462\n",
      "epoch: 12, batch: 700, loss: 146.496475\n",
      "epoch: 12, batch: 800, loss: 133.241852\n",
      "epoch: 12, batch: 900, loss: 139.320892\n",
      "epoch: 12, batch: 1000, loss: 134.247375\n",
      "epoch: 12, batch: 1100, loss: 131.999741\n",
      "epoch: 12, batch: 1200, loss: 135.181900\n",
      "epoch: 12, batch: 1300, loss: 142.982651\n",
      "epoch: 12, batch: 1400, loss: 142.533813\n",
      "epoch: 12, batch: 1500, loss: 133.217392\n",
      "epoch: 12, batch: 1600, loss: 145.366379\n",
      "epoch: 12, batch: 1700, loss: 130.757446\n",
      "epoch: 12, batch: 1800, loss: 135.721527\n",
      "epoch: 13, batch: 0, loss: 140.138504\n",
      "epoch: 13, batch: 100, loss: 141.763519\n",
      "epoch: 13, batch: 200, loss: 132.544983\n",
      "epoch: 13, batch: 300, loss: 130.082169\n",
      "epoch: 13, batch: 400, loss: 161.659119\n",
      "epoch: 13, batch: 500, loss: 154.647736\n",
      "epoch: 13, batch: 600, loss: 147.305893\n",
      "epoch: 13, batch: 700, loss: 133.194885\n",
      "epoch: 13, batch: 800, loss: 145.089630\n",
      "epoch: 13, batch: 900, loss: 126.592239\n",
      "epoch: 13, batch: 1000, loss: 147.883179\n",
      "epoch: 13, batch: 1100, loss: 125.168472\n",
      "epoch: 13, batch: 1200, loss: 130.571869\n",
      "epoch: 13, batch: 1300, loss: 125.956375\n",
      "epoch: 13, batch: 1400, loss: 137.966583\n",
      "epoch: 13, batch: 1500, loss: 136.093140\n",
      "epoch: 13, batch: 1600, loss: 152.986618\n",
      "epoch: 13, batch: 1700, loss: 142.518250\n",
      "epoch: 13, batch: 1800, loss: 124.442001\n",
      "epoch: 14, batch: 0, loss: 135.717255\n",
      "epoch: 14, batch: 100, loss: 147.331741\n",
      "epoch: 14, batch: 200, loss: 142.501328\n",
      "epoch: 14, batch: 300, loss: 139.701736\n",
      "epoch: 14, batch: 400, loss: 140.953568\n",
      "epoch: 14, batch: 500, loss: 136.573639\n",
      "epoch: 14, batch: 600, loss: 144.303833\n",
      "epoch: 14, batch: 700, loss: 146.894104\n",
      "epoch: 14, batch: 800, loss: 126.845383\n",
      "epoch: 14, batch: 900, loss: 128.274536\n",
      "epoch: 14, batch: 1000, loss: 136.502228\n",
      "epoch: 14, batch: 1100, loss: 117.827896\n",
      "epoch: 14, batch: 1200, loss: 141.111816\n",
      "epoch: 14, batch: 1300, loss: 143.984436\n",
      "epoch: 14, batch: 1400, loss: 127.585846\n",
      "epoch: 14, batch: 1500, loss: 122.083969\n",
      "epoch: 14, batch: 1600, loss: 136.242523\n",
      "epoch: 14, batch: 1700, loss: 140.915894\n",
      "epoch: 14, batch: 1800, loss: 137.010818\n",
      "epoch: 15, batch: 0, loss: 140.436096\n",
      "epoch: 15, batch: 100, loss: 143.476685\n",
      "epoch: 15, batch: 200, loss: 134.272018\n",
      "epoch: 15, batch: 300, loss: 143.183228\n",
      "epoch: 15, batch: 400, loss: 132.528580\n",
      "epoch: 15, batch: 500, loss: 142.339050\n",
      "epoch: 15, batch: 600, loss: 134.553314\n",
      "epoch: 15, batch: 700, loss: 141.865997\n",
      "epoch: 15, batch: 800, loss: 131.972305\n",
      "epoch: 15, batch: 900, loss: 138.207489\n",
      "epoch: 15, batch: 1000, loss: 123.822678\n",
      "epoch: 15, batch: 1100, loss: 125.084305\n",
      "epoch: 15, batch: 1200, loss: 145.370224\n",
      "epoch: 15, batch: 1300, loss: 136.082306\n",
      "epoch: 15, batch: 1400, loss: 137.972595\n",
      "epoch: 15, batch: 1500, loss: 129.040009\n",
      "epoch: 15, batch: 1600, loss: 141.032898\n",
      "epoch: 15, batch: 1700, loss: 130.541840\n",
      "epoch: 15, batch: 1800, loss: 138.549438\n",
      "epoch: 16, batch: 0, loss: 133.623505\n",
      "epoch: 16, batch: 100, loss: 136.824387\n",
      "epoch: 16, batch: 200, loss: 132.558929\n",
      "epoch: 16, batch: 300, loss: 141.484100\n",
      "epoch: 16, batch: 400, loss: 137.305634\n",
      "epoch: 16, batch: 500, loss: 130.512695\n",
      "epoch: 16, batch: 600, loss: 133.986435\n",
      "epoch: 16, batch: 700, loss: 141.677017\n",
      "epoch: 16, batch: 800, loss: 129.980270\n",
      "epoch: 16, batch: 900, loss: 148.602203\n",
      "epoch: 16, batch: 1000, loss: 137.654480\n",
      "epoch: 16, batch: 1100, loss: 144.027420\n",
      "epoch: 16, batch: 1200, loss: 123.894104\n",
      "epoch: 16, batch: 1300, loss: 140.261719\n",
      "epoch: 16, batch: 1400, loss: 139.648254\n",
      "epoch: 16, batch: 1500, loss: 142.346252\n",
      "epoch: 16, batch: 1600, loss: 137.921234\n",
      "epoch: 16, batch: 1700, loss: 137.370132\n",
      "epoch: 16, batch: 1800, loss: 130.382004\n",
      "epoch: 17, batch: 0, loss: 120.885483\n",
      "epoch: 17, batch: 100, loss: 141.818024\n",
      "epoch: 17, batch: 200, loss: 145.464371\n",
      "epoch: 17, batch: 300, loss: 134.418579\n",
      "epoch: 17, batch: 400, loss: 137.193893\n",
      "epoch: 17, batch: 500, loss: 148.928299\n",
      "epoch: 17, batch: 600, loss: 143.299316\n",
      "epoch: 17, batch: 700, loss: 141.421082\n",
      "epoch: 17, batch: 800, loss: 133.732330\n",
      "epoch: 17, batch: 900, loss: 135.456726\n",
      "epoch: 17, batch: 1000, loss: 128.737274\n",
      "epoch: 17, batch: 1100, loss: 152.073166\n",
      "epoch: 17, batch: 1200, loss: 123.599617\n",
      "epoch: 17, batch: 1300, loss: 135.352753\n",
      "epoch: 17, batch: 1400, loss: 130.900833\n",
      "epoch: 17, batch: 1500, loss: 120.463654\n",
      "epoch: 17, batch: 1600, loss: 126.110771\n",
      "epoch: 17, batch: 1700, loss: 142.411072\n",
      "epoch: 17, batch: 1800, loss: 146.039642\n",
      "epoch: 18, batch: 0, loss: 138.850861\n",
      "epoch: 18, batch: 100, loss: 124.110214\n",
      "epoch: 18, batch: 200, loss: 135.856628\n",
      "epoch: 18, batch: 300, loss: 132.262360\n",
      "epoch: 18, batch: 400, loss: 141.420715\n",
      "epoch: 18, batch: 500, loss: 134.751862\n",
      "epoch: 18, batch: 600, loss: 142.673523\n",
      "epoch: 18, batch: 700, loss: 136.160187\n",
      "epoch: 18, batch: 800, loss: 136.822372\n",
      "epoch: 18, batch: 900, loss: 138.371155\n",
      "epoch: 18, batch: 1000, loss: 141.507355\n",
      "epoch: 18, batch: 1100, loss: 128.743744\n",
      "epoch: 18, batch: 1200, loss: 135.770630\n",
      "epoch: 18, batch: 1300, loss: 142.894684\n",
      "epoch: 18, batch: 1400, loss: 132.874741\n",
      "epoch: 18, batch: 1500, loss: 128.622925\n",
      "epoch: 18, batch: 1600, loss: 139.759583\n",
      "epoch: 18, batch: 1700, loss: 139.254318\n",
      "epoch: 18, batch: 1800, loss: 121.855850\n",
      "epoch: 19, batch: 0, loss: 128.950134\n",
      "epoch: 19, batch: 100, loss: 123.386551\n",
      "epoch: 19, batch: 200, loss: 144.741623\n",
      "epoch: 19, batch: 300, loss: 148.533554\n",
      "epoch: 19, batch: 400, loss: 135.581970\n",
      "epoch: 19, batch: 500, loss: 130.971497\n",
      "epoch: 19, batch: 600, loss: 129.809387\n",
      "epoch: 19, batch: 700, loss: 141.543839\n",
      "epoch: 19, batch: 800, loss: 129.394287\n",
      "epoch: 19, batch: 900, loss: 136.524979\n",
      "epoch: 19, batch: 1000, loss: 146.351364\n",
      "epoch: 19, batch: 1100, loss: 132.010071\n",
      "epoch: 19, batch: 1200, loss: 139.787643\n",
      "epoch: 19, batch: 1300, loss: 151.650665\n",
      "epoch: 19, batch: 1400, loss: 130.593506\n",
      "epoch: 19, batch: 1500, loss: 149.330750\n",
      "epoch: 19, batch: 1600, loss: 123.432312\n",
      "epoch: 19, batch: 1700, loss: 145.974823\n",
      "epoch: 19, batch: 1800, loss: 133.761429\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}