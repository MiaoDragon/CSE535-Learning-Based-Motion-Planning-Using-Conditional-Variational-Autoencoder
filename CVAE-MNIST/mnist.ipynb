{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('cse535': conda)",
   "display_name": "Python 3.8.5 64-bit ('cse535': conda)",
   "metadata": {
    "interpreter": {
     "hash": "426ee9227c26455faf607ec3a590f915894f1ba5a016c6956515b9ee914a0b72"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "source": [
    "# define CVAE model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, cond_size, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.mu_network = nn.Sequential(\n",
    "            nn.Linear(input_size+cond_size, 512), nn.PReLU(), nn.Dropout(),\n",
    "            nn.Linear(512, 256), nn.PReLU(), nn.Dropout(),\n",
    "            nn.Linear(256, 32), nn.PReLU(),\n",
    "            nn.Linear(32, latent_size)\n",
    "        )\n",
    "        # we set the covariance matrix to be diag([sigma_1,...,sigma_k])\n",
    "        self.log_sigma_pow2_network = nn.Sequential(\n",
    "            nn.Linear(input_size+cond_size, 512), nn.PReLU(), nn.Dropout(),\n",
    "            nn.Linear(512, 256), nn.PReLU(), nn.Dropout(),\n",
    "            nn.Linear(256, 32), nn.PReLU(),\n",
    "            nn.Linear(32, latent_size)\n",
    "        )\n",
    "        self.output_size = latent_size\n",
    "        self.latent_size = latent_size\n",
    "        self.input_size = input_size\n",
    "        self.cond_size = cond_size\n",
    "    def forward(self, x, y):\n",
    "        # input tensor shape: BxK1, BxK2\n",
    "        input = torch.cat([x,y], dim=1)\n",
    "        mu = self.mu_network(input)\n",
    "        log_sigma_pow2 = self.log_sigma_pow2_network(input)\n",
    "        return mu, log_sigma_pow2\n",
    "\n",
    "    def sample(self, mu, log_sigma_pow2, L):\n",
    "        # given the computed mu, and sigma, obtain L samples by reparameterization\n",
    "        # draw standard normal distribution\n",
    "        # input: Bxk\n",
    "        # return: LxBxk\n",
    "        eps = torch.randn((L,len(mu),self.latent_size))\n",
    "        if log_sigma_pow2.is_cuda:\n",
    "            eps = eps.cuda()\n",
    "        eps = eps * torch.exp(log_sigma_pow2/2)\n",
    "        eps = eps + mu\n",
    "        return eps\n",
    "\n",
    "    def kl_divergence(self, mu, log_sigma_pow2):\n",
    "        # given mu and log(sigma^2), obtain the KL divergence relative to N(0,I)\n",
    "        # using formula from https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes\n",
    "        # input: BxK\n",
    "        # output: B\n",
    "        res = 1.0 / 2 * (-torch.sum(log_sigma_pow2, dim=1)-self.output_size+\\\n",
    "                         torch.sum(torch.exp(log_sigma_pow2), dim=1)+torch.sum(mu*mu, dim=1))\n",
    "        return res\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, cond_size, output_size, sigma=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.mu_network = nn.Sequential(\n",
    "            nn.Linear(latent_size+cond_size, 512), nn.PReLU(), nn.Dropout(),\n",
    "            nn.Linear(512, 256), nn.PReLU(), nn.Dropout(),\n",
    "            nn.Linear(256, 32), nn.PReLU(),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "        self.latent_size = latent_size\n",
    "        self.cond_size = cond_size\n",
    "        self.sigma = sigma\n",
    "        self.sigma_2 = sigma*sigma\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        input = torch.cat([z,y],dim=1)\n",
    "        mu = self.mu_network(input)\n",
    "        return mu\n",
    "\n",
    "    def sample(self, mu):\n",
    "        # use the computed mu to generate sample\n",
    "        # input: BxN\n",
    "        eps = torch.randn(mu.size())\n",
    "        if mu.is_cuda:\n",
    "            eps = eps.cuda()\n",
    "        eps = eps * self.sigma + mu\n",
    "        return eps\n",
    "\n",
    "    def generation_loss(self, x, mu):\n",
    "        # input:\n",
    "        # - mu: LxBxN\n",
    "        # - x: BxN\n",
    "        # output: BxN\n",
    "        # formula: 1/L * sum_z log(N(x; mu, sigma^2I))\n",
    "        #       => -1/L \\sum_z 1/2*(x-mu)^T(x-mu)/(sigma^2)\n",
    "\n",
    "        #res = - 1.0/2*(x-mu)*(x-mu)/self.sigma_2  # using normal distribution for p(x|z,y)\n",
    "        res = torch.nn.functional.binary_cross_entropy(mu, x)\n",
    "        print(res.size())\n",
    "        res = torch.sum(res, dim=2) # sum up (x-mu)^2\n",
    "        # calculate the mean w.r.t. first dimension (L)\n",
    "        res = torch.mean(res, dim=0)\n",
    "        return res\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, cond_size):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_size, cond_size, latent_size)\n",
    "        self.decoder = Decoder(latent_size, cond_size, input_size, sigma=0.1)\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        self.cond_size = cond_size\n",
    "\n",
    "    def train_forward(self, x, y, L=10):\n",
    "        # get necessary signals from the input\n",
    "        z_mu, z_log_sigma_pow2 = self.encoder(x, y)\n",
    "        # generate samples of z using the mean and variance\n",
    "        z = self.encoder.sample(z_mu, z_log_sigma_pow2, L)\n",
    "        y_extended = y.repeat(len(z),1).view(-1,self.cond_size)\n",
    "        z = z.view(-1,self.latent_size)  # B and L together first\n",
    "        if x.is_cuda:\n",
    "            z.cuda()\n",
    "        # copy y so we have shape: LxBxc\n",
    "        x_mu = self.decoder(z, y_extended).view(L,-1,self.input_size)\n",
    "        return z_mu,z_log_sigma_pow2, z, x_mu\n",
    "\n",
    "    def gen_forward(self, y):\n",
    "        # randomly sample a latent z\n",
    "        z = torch.randn(len(y),self.latent_size)\n",
    "        if y.is_cuda:\n",
    "            z = z.cuda()\n",
    "        x_mean = self.decoder(z, y)\n",
    "        x = self.decoder.sample(x_mean)\n",
    "        return x\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit testing for encoder\n",
    "# test sample\n",
    "\n",
    "# def sample(self, mu, log_sigma_pow2, L):\n",
    "#     # given the computed mu, and sigma, obtain L samples by reparameterization\n",
    "#     # draw standard normal distribution\n",
    "#     # input: Bxk\n",
    "#     # return: LxBxk\n",
    "input_size = 20\n",
    "cond_size = 2\n",
    "latent_size = 5\n",
    "encoder = Encoder(input_size, cond_size, latent_size)\n",
    "mu = torch.ones((10,5))\n",
    "sigma = torch.abs(torch.randn(10,5))\n",
    "log_sigma_pow2 = torch.log(sigma*sigma)\n",
    "L = 1000\n",
    "z = encoder.sample(mu, log_sigma_pow2, L)\n",
    "\n",
    "\n",
    "# test KL divergence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(z)\n",
    "unit_testing = False\n",
    "if unit_testing:\n",
    "    print('estimated mean: ')\n",
    "    print(torch.mean(z, dim=0))\n",
    "    print('estimated std: ')\n",
    "    print(torch.std(z, dim=0))\n",
    "    print('true mean:')\n",
    "    print(mu)\n",
    "    print('true std:')\n",
    "    print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "def idx2onehot(idx, n):\n",
    "\n",
    "    assert torch.max(idx).item() < n\n",
    "    if idx.dim() == 1:\n",
    "        idx = idx.unsqueeze(1)\n",
    "\n",
    "    onehot = torch.zeros(idx.size(0), n)\n",
    "    onehot.scatter_(1, idx, 1)\n",
    "\n",
    "    return onehot\n",
    "\n",
    "def main():\n",
    "    batch_size = 32\n",
    "    input_size = 28*28\n",
    "    latent_size = 16\n",
    "    cond_size = 10\n",
    "    learning_rate = 0.001\n",
    "    num_epoch = 20\n",
    "    print_every = 100\n",
    "\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "    dataset = MNIST(\n",
    "            root='data', train=True, transform=transforms.ToTensor(),\n",
    "            download=True)\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    cvae = CVAE(input_size, latent_size, cond_size)\n",
    "    cvae.cuda()\n",
    "    optimizer = torch.optim.Adam(cvae.parameters(), learning_rate)\n",
    "\n",
    "    logs = defaultdict(list)\n",
    "    print('start training...')\n",
    "    for epoch in range(num_epoch):\n",
    "        for iter, (x, y) in enumerate(data_loader):\n",
    "            y = idx2onehot(y, n=10)\n",
    "            x = x.view(-1,28*28)\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            #x, y = x.to(device), y.to(device)\n",
    "            z_mu,z_log_sigma_pow2, z, x_mu = cvae.train_forward(x, y, L=10)\n",
    "            kl_divergence = cvae.encoder.kl_divergence(z_mu, z_log_sigma_pow2)\n",
    "            generation_loss = cvae.decoder.generation_loss(x, x_mu)\n",
    "            loss_i = -generation_loss + kl_divergence\n",
    "            loss_i = torch.mean(loss_i)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_i.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            logs['loss'].append(loss_i.item())\n",
    "            if iter % print_every == 0:\n",
    "                print('epoch: %d, batch: %d, loss: %f' % (epoch, iter, loss_i.item()))\n",
    "                # save the reconstructed inference\n",
    "\n",
    "                y = torch.arange(0, 10).long().unsqueeze(1)\n",
    "                y_onehot = idx2onehot(y, n=10)\n",
    "                y_onehot = y_onehot.cuda()\n",
    "                x = cvae.gen_forward(y_onehot)\n",
    "\n",
    "                plt.figure()\n",
    "                plt.figure(figsize=(5, 10))\n",
    "                for p in range(10):\n",
    "                    plt.subplot(5, 2, p+1)\n",
    "                    plt.text(\n",
    "                        0, 0, \"y={:d}\".format(y[p].item()), color='black',\n",
    "                        backgroundcolor='white', fontsize=8)\n",
    "                    plt.imshow(x[p].cpu().view(28, 28).data.numpy())\n",
    "                    plt.axis('off')\n",
    "                fig_root = 'plots'\n",
    "                os.makedirs(os.path.join(fig_root), exist_ok=True)\n",
    "                plt.savefig(\n",
    "                    os.path.join(fig_root,\n",
    "                                 \"epoch{:d}batch{:d}.png\".format(epoch, iter)),\n",
    "                    dpi=300)\n",
    "                plt.clf()\n",
    "                plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start training...\n",
      "epoch: 0, batch: 0, loss: 5005.858887\n",
      "epoch: 0, batch: 100, loss: 2641.375000\n",
      "epoch: 0, batch: 200, loss: 2393.677246\n",
      "epoch: 0, batch: 300, loss: 1872.641235\n",
      "epoch: 0, batch: 400, loss: 2084.156982\n",
      "epoch: 0, batch: 500, loss: 1831.373047\n",
      "epoch: 0, batch: 600, loss: 1831.557129\n",
      "epoch: 0, batch: 700, loss: 1817.189453\n",
      "epoch: 0, batch: 800, loss: 1887.752197\n",
      "epoch: 0, batch: 900, loss: 1692.385986\n",
      "epoch: 0, batch: 1000, loss: 1732.146240\n",
      "epoch: 0, batch: 1100, loss: 1718.848389\n",
      "epoch: 0, batch: 1200, loss: 1586.529419\n",
      "epoch: 0, batch: 1300, loss: 1691.145508\n",
      "epoch: 0, batch: 1400, loss: 1713.337891\n",
      "epoch: 0, batch: 1500, loss: 1594.228516\n",
      "epoch: 0, batch: 1600, loss: 1519.271606\n",
      "epoch: 0, batch: 1700, loss: 1446.788574\n",
      "epoch: 0, batch: 1800, loss: 1451.198242\n",
      "epoch: 1, batch: 0, loss: 1282.422852\n",
      "epoch: 1, batch: 100, loss: 1399.795654\n",
      "epoch: 1, batch: 200, loss: 1428.735352\n",
      "epoch: 1, batch: 300, loss: 1379.493408\n",
      "epoch: 1, batch: 400, loss: 1443.405151\n",
      "epoch: 1, batch: 500, loss: 1483.655029\n",
      "epoch: 1, batch: 600, loss: 1462.627686\n",
      "epoch: 1, batch: 700, loss: 1310.271484\n",
      "epoch: 1, batch: 800, loss: 1361.227051\n",
      "epoch: 1, batch: 900, loss: 1473.038574\n",
      "epoch: 1, batch: 1000, loss: 1484.931641\n",
      "epoch: 1, batch: 1100, loss: 1678.001221\n",
      "epoch: 1, batch: 1200, loss: 1271.244019\n",
      "epoch: 1, batch: 1300, loss: 1336.276245\n",
      "epoch: 1, batch: 1400, loss: 1350.466553\n",
      "epoch: 1, batch: 1500, loss: 1369.567383\n",
      "epoch: 1, batch: 1600, loss: 1455.906250\n",
      "epoch: 1, batch: 1700, loss: 1273.201782\n",
      "epoch: 1, batch: 1800, loss: 1362.857666\n",
      "epoch: 2, batch: 0, loss: 1315.130859\n",
      "epoch: 2, batch: 100, loss: 1496.904541\n",
      "epoch: 2, batch: 200, loss: 1390.765625\n",
      "epoch: 2, batch: 300, loss: 1214.784424\n",
      "epoch: 2, batch: 400, loss: 1256.399170\n",
      "epoch: 2, batch: 500, loss: 1409.775635\n",
      "epoch: 2, batch: 600, loss: 1291.071289\n",
      "epoch: 2, batch: 700, loss: 1190.996582\n",
      "epoch: 2, batch: 800, loss: 1240.875244\n",
      "epoch: 2, batch: 900, loss: 1330.496338\n",
      "epoch: 2, batch: 1000, loss: 1055.385742\n",
      "epoch: 2, batch: 1100, loss: 1275.063721\n",
      "epoch: 2, batch: 1200, loss: 1304.881592\n",
      "epoch: 2, batch: 1300, loss: 1475.953369\n",
      "epoch: 2, batch: 1400, loss: 1322.541016\n",
      "epoch: 2, batch: 1500, loss: 1374.497559\n",
      "epoch: 2, batch: 1600, loss: 1154.343262\n",
      "epoch: 2, batch: 1700, loss: 1300.715454\n",
      "epoch: 2, batch: 1800, loss: 1212.988525\n",
      "epoch: 3, batch: 0, loss: 1373.547119\n",
      "epoch: 3, batch: 100, loss: 1203.319336\n",
      "epoch: 3, batch: 200, loss: 1201.665649\n",
      "epoch: 3, batch: 300, loss: 1209.933594\n",
      "epoch: 3, batch: 400, loss: 1219.228027\n",
      "epoch: 3, batch: 500, loss: 1337.804688\n",
      "epoch: 3, batch: 600, loss: 1079.802856\n",
      "epoch: 3, batch: 700, loss: 1336.943359\n",
      "epoch: 3, batch: 800, loss: 1292.595947\n",
      "epoch: 3, batch: 900, loss: 1433.009766\n",
      "epoch: 3, batch: 1000, loss: 1210.287231\n",
      "epoch: 3, batch: 1100, loss: 1081.409424\n",
      "epoch: 3, batch: 1200, loss: 1295.052856\n",
      "epoch: 3, batch: 1300, loss: 1109.555786\n",
      "epoch: 3, batch: 1400, loss: 1188.954102\n",
      "epoch: 3, batch: 1500, loss: 1093.198730\n",
      "epoch: 3, batch: 1600, loss: 1194.568848\n",
      "epoch: 3, batch: 1700, loss: 1254.348511\n",
      "epoch: 3, batch: 1800, loss: 1159.929688\n",
      "epoch: 4, batch: 0, loss: 1295.543213\n",
      "epoch: 4, batch: 100, loss: 1299.765625\n",
      "epoch: 4, batch: 200, loss: 1205.787354\n",
      "epoch: 4, batch: 300, loss: 1050.219604\n",
      "epoch: 4, batch: 400, loss: 1333.159180\n",
      "epoch: 4, batch: 500, loss: 1275.235352\n",
      "epoch: 4, batch: 600, loss: 1250.876221\n",
      "epoch: 4, batch: 700, loss: 1229.890869\n",
      "epoch: 4, batch: 800, loss: 1253.995117\n",
      "epoch: 4, batch: 900, loss: 1287.139526\n",
      "epoch: 4, batch: 1000, loss: 1285.656250\n",
      "epoch: 4, batch: 1100, loss: 1197.261719\n",
      "epoch: 4, batch: 1200, loss: 1182.674561\n",
      "epoch: 4, batch: 1300, loss: 1214.715088\n",
      "epoch: 4, batch: 1400, loss: 1285.657715\n",
      "epoch: 4, batch: 1500, loss: 1288.840454\n",
      "epoch: 4, batch: 1600, loss: 1241.378662\n",
      "epoch: 4, batch: 1700, loss: 1194.403076\n",
      "epoch: 4, batch: 1800, loss: 1352.114502\n",
      "epoch: 5, batch: 0, loss: 1264.389526\n",
      "epoch: 5, batch: 100, loss: 1260.210083\n",
      "epoch: 5, batch: 200, loss: 1111.784058\n",
      "epoch: 5, batch: 300, loss: 1249.420532\n",
      "epoch: 5, batch: 400, loss: 1198.582886\n",
      "epoch: 5, batch: 500, loss: 1235.028809\n",
      "epoch: 5, batch: 600, loss: 1157.649170\n",
      "epoch: 5, batch: 700, loss: 1230.038452\n",
      "epoch: 5, batch: 800, loss: 1153.625854\n",
      "epoch: 5, batch: 900, loss: 1095.666016\n",
      "epoch: 5, batch: 1000, loss: 1164.679565\n",
      "epoch: 5, batch: 1100, loss: 1163.364258\n",
      "epoch: 5, batch: 1200, loss: 1236.245728\n",
      "epoch: 5, batch: 1300, loss: 1115.547607\n",
      "epoch: 5, batch: 1400, loss: 1220.733032\n",
      "epoch: 5, batch: 1500, loss: 1252.058594\n",
      "epoch: 5, batch: 1600, loss: 1341.703857\n",
      "epoch: 5, batch: 1700, loss: 1195.433472\n",
      "epoch: 5, batch: 1800, loss: 1216.288208\n",
      "epoch: 6, batch: 0, loss: 1347.645386\n",
      "epoch: 6, batch: 100, loss: 1197.310913\n",
      "epoch: 6, batch: 200, loss: 1219.497070\n",
      "epoch: 6, batch: 300, loss: 1158.500488\n",
      "epoch: 6, batch: 400, loss: 1301.910156\n",
      "epoch: 6, batch: 500, loss: 1448.204346\n",
      "epoch: 6, batch: 600, loss: 1267.506348\n",
      "epoch: 6, batch: 700, loss: 1117.076172\n",
      "epoch: 6, batch: 800, loss: 1146.584961\n",
      "epoch: 6, batch: 900, loss: 1177.471069\n",
      "epoch: 6, batch: 1000, loss: 1117.262695\n",
      "epoch: 6, batch: 1100, loss: 1215.231934\n",
      "epoch: 6, batch: 1200, loss: 1227.181030\n",
      "epoch: 6, batch: 1300, loss: 1305.417725\n",
      "epoch: 6, batch: 1400, loss: 1273.767700\n",
      "epoch: 6, batch: 1500, loss: 1121.098877\n",
      "epoch: 6, batch: 1600, loss: 1148.804932\n",
      "epoch: 6, batch: 1700, loss: 1139.876221\n",
      "epoch: 6, batch: 1800, loss: 1091.367188\n",
      "epoch: 7, batch: 0, loss: 1215.439941\n",
      "epoch: 7, batch: 100, loss: 1225.480347\n",
      "epoch: 7, batch: 200, loss: 1237.772095\n",
      "epoch: 7, batch: 300, loss: 1269.947021\n",
      "epoch: 7, batch: 400, loss: 1056.846191\n",
      "epoch: 7, batch: 500, loss: 1047.241943\n",
      "epoch: 7, batch: 600, loss: 1253.244751\n",
      "epoch: 7, batch: 700, loss: 1243.221680\n",
      "epoch: 7, batch: 800, loss: 1183.961914\n",
      "epoch: 7, batch: 900, loss: 1251.033325\n",
      "epoch: 7, batch: 1000, loss: 1327.292114\n",
      "epoch: 7, batch: 1100, loss: 1203.317871\n",
      "epoch: 7, batch: 1200, loss: 1295.340332\n",
      "epoch: 7, batch: 1300, loss: 1191.325928\n",
      "epoch: 7, batch: 1400, loss: 1357.824707\n",
      "epoch: 7, batch: 1500, loss: 1335.163086\n",
      "epoch: 7, batch: 1600, loss: 1256.514160\n",
      "epoch: 7, batch: 1700, loss: 1148.747192\n",
      "epoch: 7, batch: 1800, loss: 1195.002319\n",
      "epoch: 8, batch: 0, loss: 1354.962646\n",
      "epoch: 8, batch: 100, loss: 1206.771240\n",
      "epoch: 8, batch: 200, loss: 1237.311035\n",
      "epoch: 8, batch: 300, loss: 1263.776611\n",
      "epoch: 8, batch: 400, loss: 1242.447998\n",
      "epoch: 8, batch: 500, loss: 1355.741211\n",
      "epoch: 8, batch: 600, loss: 1310.260498\n",
      "epoch: 8, batch: 700, loss: 1130.101807\n",
      "epoch: 8, batch: 800, loss: 1245.979858\n",
      "epoch: 8, batch: 900, loss: 1188.427490\n",
      "epoch: 8, batch: 1000, loss: 1115.984009\n",
      "epoch: 8, batch: 1100, loss: 1368.989990\n",
      "epoch: 8, batch: 1200, loss: 1132.938110\n",
      "epoch: 8, batch: 1300, loss: 1275.065186\n",
      "epoch: 8, batch: 1400, loss: 1251.219482\n",
      "epoch: 8, batch: 1500, loss: 1298.816772\n",
      "epoch: 8, batch: 1600, loss: 1172.502686\n",
      "epoch: 8, batch: 1700, loss: 1131.842529\n",
      "epoch: 8, batch: 1800, loss: 1164.805786\n",
      "epoch: 9, batch: 0, loss: 1192.025757\n",
      "epoch: 9, batch: 100, loss: 1239.711914\n",
      "epoch: 9, batch: 200, loss: 1167.897095\n",
      "epoch: 9, batch: 300, loss: 1196.250488\n",
      "epoch: 9, batch: 400, loss: 1288.550537\n",
      "epoch: 9, batch: 500, loss: 1264.347900\n",
      "epoch: 9, batch: 600, loss: 1177.238770\n",
      "epoch: 9, batch: 700, loss: 1178.429077\n",
      "epoch: 9, batch: 800, loss: 1118.091553\n",
      "epoch: 9, batch: 900, loss: 1043.791504\n",
      "epoch: 9, batch: 1000, loss: 1236.236572\n",
      "epoch: 9, batch: 1100, loss: 1145.453125\n",
      "epoch: 9, batch: 1200, loss: 1150.818604\n",
      "epoch: 9, batch: 1300, loss: 1246.478394\n",
      "epoch: 9, batch: 1400, loss: 1290.208374\n",
      "epoch: 9, batch: 1500, loss: 1298.344727\n",
      "epoch: 9, batch: 1600, loss: 1343.769775\n",
      "epoch: 9, batch: 1700, loss: 1226.462036\n",
      "epoch: 9, batch: 1800, loss: 1272.603394\n",
      "epoch: 10, batch: 0, loss: 1101.468872\n",
      "epoch: 10, batch: 100, loss: 1048.752441\n",
      "epoch: 10, batch: 200, loss: 1385.552368\n",
      "epoch: 10, batch: 300, loss: 1080.047607\n",
      "epoch: 10, batch: 400, loss: 1127.365479\n",
      "epoch: 10, batch: 500, loss: 1241.979858\n",
      "epoch: 10, batch: 600, loss: 1297.583008\n",
      "epoch: 10, batch: 700, loss: 1170.742188\n",
      "epoch: 10, batch: 800, loss: 1288.913818\n",
      "epoch: 10, batch: 900, loss: 1149.441406\n",
      "epoch: 10, batch: 1000, loss: 1304.339355\n",
      "epoch: 10, batch: 1100, loss: 1181.708008\n",
      "epoch: 10, batch: 1200, loss: 1252.973022\n",
      "epoch: 10, batch: 1300, loss: 1169.736206\n",
      "epoch: 10, batch: 1400, loss: 1194.208862\n",
      "epoch: 10, batch: 1500, loss: 1308.357300\n",
      "epoch: 10, batch: 1600, loss: 1182.035156\n",
      "epoch: 10, batch: 1700, loss: 1279.992188\n",
      "epoch: 10, batch: 1800, loss: 1194.562988\n",
      "epoch: 11, batch: 0, loss: 1094.930054\n",
      "epoch: 11, batch: 100, loss: 1257.080444\n",
      "epoch: 11, batch: 200, loss: 1196.459595\n",
      "epoch: 11, batch: 300, loss: 1217.184326\n",
      "epoch: 11, batch: 400, loss: 1224.179443\n",
      "epoch: 11, batch: 500, loss: 1202.962646\n",
      "epoch: 11, batch: 600, loss: 1231.818848\n",
      "epoch: 11, batch: 700, loss: 1122.019775\n",
      "epoch: 11, batch: 800, loss: 1253.157227\n",
      "epoch: 11, batch: 900, loss: 1126.835938\n",
      "epoch: 11, batch: 1000, loss: 1245.250488\n",
      "epoch: 11, batch: 1100, loss: 1084.913818\n",
      "epoch: 11, batch: 1200, loss: 1228.259766\n",
      "epoch: 11, batch: 1300, loss: 1165.054321\n",
      "epoch: 11, batch: 1400, loss: 1243.421265\n",
      "epoch: 11, batch: 1500, loss: 1293.089478\n",
      "epoch: 11, batch: 1600, loss: 1142.229736\n",
      "epoch: 11, batch: 1700, loss: 1234.979492\n",
      "epoch: 11, batch: 1800, loss: 1174.967285\n",
      "epoch: 12, batch: 0, loss: 1256.125488\n",
      "epoch: 12, batch: 100, loss: 1214.286377\n",
      "epoch: 12, batch: 200, loss: 1405.652588\n",
      "epoch: 12, batch: 300, loss: 1123.060913\n",
      "epoch: 12, batch: 400, loss: 1134.380127\n",
      "epoch: 12, batch: 500, loss: 1064.114136\n",
      "epoch: 12, batch: 600, loss: 1237.226196\n",
      "epoch: 12, batch: 700, loss: 1379.180908\n",
      "epoch: 12, batch: 800, loss: 1197.628662\n",
      "epoch: 12, batch: 900, loss: 1211.451416\n",
      "epoch: 12, batch: 1000, loss: 1132.143555\n",
      "epoch: 12, batch: 1100, loss: 1108.262939\n",
      "epoch: 12, batch: 1200, loss: 1347.608154\n",
      "epoch: 12, batch: 1300, loss: 1367.693359\n",
      "epoch: 12, batch: 1400, loss: 1175.521973\n",
      "epoch: 12, batch: 1500, loss: 1194.772217\n",
      "epoch: 12, batch: 1600, loss: 1228.267334\n",
      "epoch: 12, batch: 1700, loss: 1255.333496\n",
      "epoch: 12, batch: 1800, loss: 1249.632080\n",
      "epoch: 13, batch: 0, loss: 1161.302490\n",
      "epoch: 13, batch: 100, loss: 1092.212769\n",
      "epoch: 13, batch: 200, loss: 1093.128174\n",
      "epoch: 13, batch: 300, loss: 1182.206177\n",
      "epoch: 13, batch: 400, loss: 1241.769531\n",
      "epoch: 13, batch: 500, loss: 1251.725586\n",
      "epoch: 13, batch: 600, loss: 1291.321289\n",
      "epoch: 13, batch: 700, loss: 1137.715332\n",
      "epoch: 13, batch: 800, loss: 1260.860352\n",
      "epoch: 13, batch: 900, loss: 1134.357300\n",
      "epoch: 13, batch: 1000, loss: 1216.187744\n",
      "epoch: 13, batch: 1100, loss: 1192.189453\n",
      "epoch: 13, batch: 1200, loss: 1234.899658\n",
      "epoch: 13, batch: 1300, loss: 1254.567139\n",
      "epoch: 13, batch: 1400, loss: 1115.977539\n",
      "epoch: 13, batch: 1500, loss: 1254.041260\n",
      "epoch: 13, batch: 1600, loss: 1144.298218\n",
      "epoch: 13, batch: 1700, loss: 1190.850586\n",
      "epoch: 13, batch: 1800, loss: 1188.581299\n",
      "epoch: 14, batch: 0, loss: 1148.397217\n",
      "epoch: 14, batch: 100, loss: 1237.224854\n",
      "epoch: 14, batch: 200, loss: 1241.996826\n",
      "epoch: 14, batch: 300, loss: 1175.831055\n",
      "epoch: 14, batch: 400, loss: 1164.541382\n",
      "epoch: 14, batch: 500, loss: 1292.026611\n",
      "epoch: 14, batch: 600, loss: 1144.524414\n",
      "epoch: 14, batch: 700, loss: 1202.524658\n",
      "epoch: 14, batch: 800, loss: 1185.578613\n",
      "epoch: 14, batch: 900, loss: 1115.037476\n",
      "epoch: 14, batch: 1000, loss: 1231.267578\n",
      "epoch: 14, batch: 1100, loss: 1223.828369\n",
      "epoch: 14, batch: 1200, loss: 1193.078613\n",
      "epoch: 14, batch: 1300, loss: 1177.116089\n",
      "epoch: 14, batch: 1400, loss: 1347.079102\n",
      "epoch: 14, batch: 1500, loss: 1195.383301\n",
      "epoch: 14, batch: 1600, loss: 1174.602539\n",
      "epoch: 14, batch: 1700, loss: 1293.571289\n",
      "epoch: 14, batch: 1800, loss: 1270.850220\n",
      "epoch: 15, batch: 0, loss: 1416.706421\n",
      "epoch: 15, batch: 100, loss: 1133.973877\n",
      "epoch: 15, batch: 200, loss: 1154.245605\n",
      "epoch: 15, batch: 300, loss: 1252.746826\n",
      "epoch: 15, batch: 400, loss: 1200.849121\n",
      "epoch: 15, batch: 500, loss: 1048.560547\n",
      "epoch: 15, batch: 600, loss: 1215.800537\n",
      "epoch: 15, batch: 700, loss: 1288.082520\n",
      "epoch: 15, batch: 800, loss: 1153.938721\n",
      "epoch: 15, batch: 900, loss: 1239.517578\n",
      "epoch: 15, batch: 1000, loss: 1194.963867\n",
      "epoch: 15, batch: 1100, loss: 1160.295044\n",
      "epoch: 15, batch: 1200, loss: 1204.970459\n",
      "epoch: 15, batch: 1300, loss: 1200.759888\n",
      "epoch: 15, batch: 1400, loss: 1310.535156\n",
      "epoch: 15, batch: 1500, loss: 1253.005371\n",
      "epoch: 15, batch: 1600, loss: 1117.059570\n",
      "epoch: 15, batch: 1700, loss: 1196.833618\n",
      "epoch: 15, batch: 1800, loss: 1044.751953\n",
      "epoch: 16, batch: 0, loss: 1188.795776\n",
      "epoch: 16, batch: 100, loss: 1205.050415\n",
      "epoch: 16, batch: 200, loss: 1124.153198\n",
      "epoch: 16, batch: 300, loss: 1272.391846\n",
      "epoch: 16, batch: 400, loss: 1338.129883\n",
      "epoch: 16, batch: 500, loss: 1334.646729\n",
      "epoch: 16, batch: 600, loss: 1015.497681\n",
      "epoch: 16, batch: 700, loss: 1114.165039\n",
      "epoch: 16, batch: 800, loss: 1181.407349\n",
      "epoch: 16, batch: 900, loss: 1131.135254\n",
      "epoch: 16, batch: 1000, loss: 1151.026489\n",
      "epoch: 16, batch: 1100, loss: 1308.072510\n",
      "epoch: 16, batch: 1200, loss: 1215.879761\n",
      "epoch: 16, batch: 1300, loss: 1190.322754\n",
      "epoch: 16, batch: 1400, loss: 1225.433838\n",
      "epoch: 16, batch: 1500, loss: 1129.395020\n",
      "epoch: 16, batch: 1600, loss: 1120.440918\n",
      "epoch: 16, batch: 1700, loss: 1177.854370\n",
      "epoch: 16, batch: 1800, loss: 1256.437134\n",
      "epoch: 17, batch: 0, loss: 1204.758301\n",
      "epoch: 17, batch: 100, loss: 1266.250000\n",
      "epoch: 17, batch: 200, loss: 1322.789917\n",
      "epoch: 17, batch: 300, loss: 1320.366455\n",
      "epoch: 17, batch: 400, loss: 1200.735107\n",
      "epoch: 17, batch: 500, loss: 1171.466553\n",
      "epoch: 17, batch: 600, loss: 1163.979736\n",
      "epoch: 17, batch: 700, loss: 1179.512695\n",
      "epoch: 17, batch: 800, loss: 1113.563232\n",
      "epoch: 17, batch: 900, loss: 1329.348511\n",
      "epoch: 17, batch: 1000, loss: 1334.265625\n",
      "epoch: 17, batch: 1100, loss: 1134.677246\n",
      "epoch: 17, batch: 1200, loss: 1047.519653\n",
      "epoch: 17, batch: 1300, loss: 1170.841064\n",
      "epoch: 17, batch: 1400, loss: 1268.762573\n",
      "epoch: 17, batch: 1500, loss: 1079.914551\n",
      "epoch: 17, batch: 1600, loss: 1167.923828\n",
      "epoch: 17, batch: 1700, loss: 1230.197510\n",
      "epoch: 17, batch: 1800, loss: 1056.078857\n",
      "epoch: 18, batch: 0, loss: 1029.793945\n",
      "epoch: 18, batch: 100, loss: 1296.568604\n",
      "epoch: 18, batch: 200, loss: 1224.988770\n",
      "epoch: 18, batch: 300, loss: 1095.993652\n",
      "epoch: 18, batch: 400, loss: 1198.779053\n",
      "epoch: 18, batch: 500, loss: 1215.806641\n",
      "epoch: 18, batch: 600, loss: 1127.121704\n",
      "epoch: 18, batch: 700, loss: 1179.359985\n",
      "epoch: 18, batch: 800, loss: 1046.772949\n",
      "epoch: 18, batch: 900, loss: 1216.328125\n",
      "epoch: 18, batch: 1000, loss: 1291.953369\n",
      "epoch: 18, batch: 1100, loss: 1111.522461\n",
      "epoch: 18, batch: 1200, loss: 1199.274780\n",
      "epoch: 18, batch: 1300, loss: 1141.616455\n",
      "epoch: 18, batch: 1400, loss: 1208.057251\n",
      "epoch: 18, batch: 1500, loss: 1199.697754\n",
      "epoch: 18, batch: 1600, loss: 1195.660400\n",
      "epoch: 18, batch: 1700, loss: 1225.157471\n",
      "epoch: 18, batch: 1800, loss: 1121.058228\n",
      "epoch: 19, batch: 0, loss: 1202.414795\n",
      "epoch: 19, batch: 100, loss: 1202.900879\n",
      "epoch: 19, batch: 200, loss: 1177.182251\n",
      "epoch: 19, batch: 300, loss: 1125.067383\n",
      "epoch: 19, batch: 400, loss: 1255.799927\n",
      "epoch: 19, batch: 500, loss: 1208.116211\n",
      "epoch: 19, batch: 600, loss: 1166.392944\n",
      "epoch: 19, batch: 700, loss: 1347.144043\n",
      "epoch: 19, batch: 800, loss: 1112.696533\n",
      "epoch: 19, batch: 900, loss: 1123.403320\n",
      "epoch: 19, batch: 1000, loss: 1319.512695\n",
      "epoch: 19, batch: 1100, loss: 1129.005615\n",
      "epoch: 19, batch: 1200, loss: 1080.890747\n",
      "epoch: 19, batch: 1300, loss: 1123.755981\n",
      "epoch: 19, batch: 1400, loss: 1196.854492\n",
      "epoch: 19, batch: 1500, loss: 1118.770020\n",
      "epoch: 19, batch: 1600, loss: 1146.077271\n",
      "epoch: 19, batch: 1700, loss: 1192.799561\n",
      "epoch: 19, batch: 1800, loss: 1241.345947\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}