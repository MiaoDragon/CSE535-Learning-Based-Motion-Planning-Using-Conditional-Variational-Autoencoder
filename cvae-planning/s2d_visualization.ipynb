{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the trained model\n",
    "\n",
    "import sys\n",
    "#sys.path.append('deps/sparse_rrt')\n",
    "sys.path.append('.')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import importlib\n",
    "#from model import ae_s2d\n",
    "#from model import cvae_s2d_model1 as cvae_s2d\n",
    "\n",
    "from model.SMPNet import SMPNet\n",
    "from model.SMPPathNet import SMPPathNet\n",
    "from model.SMPPathWithPriorNet import SMPPathWithPriorNet\n",
    "\n",
    "#from tools import data_loader\n",
    "from tools.utility import *\n",
    "#from plan_utility import cart_pole, cart_pole_obs, pendulum, acrobot_obs, car_obs\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(param_name='cvae_s2d_param3.yaml', param_path='param/train/')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-2731af580cbe>:16: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  param = yaml.load(param_f)\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "# try:\n",
    "#     from yaml import CLoader as Loader, CDumper as Dumper\n",
    "# except ImportError:\n",
    "#     from yaml import Loader, Dumper\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# for training\n",
    "parser.add_argument('--param_path', type=str, default='param/train/',help='path for loading training param')\n",
    "parser.add_argument('--param_name', type=str, default=\"cvae_s2d_param3.yaml\")\n",
    "\n",
    "# parse the parameter file\n",
    "args = parser.parse_args(\"\")\n",
    "print(args)\n",
    "param_f = open(args.param_path+args.param_name, 'r')\n",
    "param = yaml.load(param_f)\n",
    "param = DictDot(param)\n",
    "args = param\n",
    "args.start_epoch = 3\n",
    "args.s = 0\n",
    "args.sp = 4000\n",
    "args.start_iter =  6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the output of one encoder\n",
      "343\n",
      "SMPPathWithPriorNet\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(args.device)\n",
    "# environment setting\n",
    "cae = None\n",
    "mlp = None\n",
    "\n",
    "# load net\n",
    "# load previously trained model if start epoch > 0\n",
    "\n",
    "# dynamically import model\n",
    "ae_module = importlib.import_module('model.ae_%s_model_%d' % (args.env_type, args.model_id))\n",
    "cvae_module = importlib.import_module('model.cvae_%s_model_%d' % (args.env_type, args.model_id))\n",
    "e_net = ae_module.Encoder(input_size=args.e_net_input_size, output_size=args.e_net_output_size)\n",
    "cvae = cvae_module.CVAE(input_size=args.input_size, latent_size=args.latent_size, cond_size=args.cond_size)\n",
    "\n",
    "data_loader = importlib.import_module('tools.data_loader_%s' % (args.env_type))\n",
    "plan_util = importlib.import_module('plan_util.%s' % (args.env_type))\n",
    "normalize = plan_util.normalize\n",
    "unnormalize = plan_util.unnormalize\n",
    "\n",
    "if args.model_type == \"SMPNet\":\n",
    "    smpnet = SMPNet(e_net, cvae)\n",
    "elif args.model_type == \"SMPPathNet\":\n",
    "    smpnet = SMPPathNet(e_net, cvae)\n",
    "elif args.model_type == \"SMPPathWithPriorNet\":\n",
    "    smpnet = SMPPathWithPriorNet(e_net, cvae)\n",
    "\n",
    "\n",
    "model_dir = args.model_dir + '%s/%s/model_%d/param_%s/' % (args.env_type, args.model_type, args.model_id, args.param_name)\n",
    "model_path='smpnet_epoch_%d_iter_%d.pkl' %(args.start_epoch, args.start_iter)\n",
    "torch_seed, np_seed, py_seed = 0, 0, 0\n",
    "if args.start_epoch > 0:\n",
    "    load_net_state(smpnet, os.path.join(model_dir, model_path))\n",
    "    torch_seed, np_seed, py_seed = load_seed(os.path.join(model_dir, model_path))\n",
    "    # set seed after loading\n",
    "    torch.manual_seed(torch_seed)\n",
    "    np.random.seed(np_seed)\n",
    "    random.seed(py_seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    smpnet.cuda()\n",
    "    smpnet.cvae.cuda()\n",
    "    smpnet.e_net.cuda()\n",
    "    if args.opt == 'Adagrad':\n",
    "        smpnet.set_opt(torch.optim.Adagrad, lr=args.learning_rate)\n",
    "    elif args.opt == 'Adam':\n",
    "        smpnet.set_opt(torch.optim.Adam, lr=args.learning_rate)\n",
    "    elif args.opt == 'SGD':\n",
    "        smpnet.set_opt(torch.optim.SGD, lr=args.learning_rate, momentum=0.9)\n",
    "    elif args.opt == 'ASGD':\n",
    "        smpnet.set_opt(torch.optim.ASGD, lr=args.learning_rate)\n",
    "if args.start_epoch > 0:\n",
    "    load_opt_state(smpnet, os.path.join(model_dir, model_path))\n",
    "data_folder = args.data_folder+args.env_type+'/'\n",
    "print(args.model_type)\n",
    "load_dis_ratio = None\n",
    "if args.model_type == \"SMPPathNet\":\n",
    "    load_dis_ratio = True\n",
    "elif args.model_type == \"SMPPathWithPriorNet\":\n",
    "    load_dis_ratio = True\n",
    "elif args.model_type == \"SMPNet\":\n",
    "    load_dis_ratio = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.006213\n"
     ]
    }
   ],
   "source": [
    "# test to see the error\n",
    "waypoint_dataset, cond_dataset, obs, env_indices \\\n",
    "         = data_loader.load_train_dataset(N=10, NP=200, s=0, sp=0,\n",
    "                                            data_folder=data_folder, load_dis_ratio=load_dis_ratio)\n",
    "cond_dataset = np.array(cond_dataset)\n",
    "# randomize the dataset before training\n",
    "data=list(zip(waypoint_dataset, cond_dataset, env_indices))\n",
    "random.shuffle(data)\n",
    "waypoint_dataset,cond_dataset,env_indices=list(zip(*data))\n",
    "waypoint_dataset = list(waypoint_dataset)\n",
    "cond_dataset = list(cond_dataset)\n",
    "env_indices = list(env_indices)\n",
    "waypoint_dataset = np.array(waypoint_dataset)\n",
    "cond_dataset = np.array(cond_dataset)\n",
    "env_indices = np.array(env_indices)\n",
    "\n",
    "smpnet.eval()\n",
    "epoch_val_num = 2048\n",
    "waypoint_dataset_i = waypoint_dataset[:epoch_val_num]\n",
    "cond_dataset_i = cond_dataset[:epoch_val_num]\n",
    "env_indices_i = env_indices[:epoch_val_num]\n",
    "bi = waypoint_dataset_i\n",
    "bi = torch.FloatTensor(bi)\n",
    "bi = normalize(bi, args.world_size)\n",
    "bi=to_var(bi)\n",
    "\n",
    "cond_dataset_i = torch.FloatTensor(cond_dataset_i)\n",
    "cond_dataset_i = normalize(cond_dataset_i, args.world_size)  # assume the first Sx2 are start and goal\n",
    "cond_dataset_i = to_var(cond_dataset_i)\n",
    "\n",
    "bobs = obs[env_indices_i].astype(np.float32)\n",
    "bobs = torch.FloatTensor(bobs)\n",
    "bobs = to_var(bobs)\n",
    "loss = smpnet.loss(bi, smpnet.train_forward(bi, cond_dataset_i, bobs), beta=args.beta)\n",
    "loss = torch.mean(loss)\n",
    "print('mean loss: %f' % (loss.cpu().item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.006243\n",
      "generation loss: 0.006284\n",
      "KL loss: 0.000010\n",
      "reconstruction: \n",
      "tensor([[[-0.5404,  0.6372],\n",
      "         [ 0.3390,  0.8312],\n",
      "         [-0.0448,  0.7321],\n",
      "         [ 0.4369,  0.8837],\n",
      "         [-0.2810,  0.6913],\n",
      "         [-0.8182,  0.6717]]], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "torch.Size([1, 6, 2])\n",
      "ground truth:\n",
      "tensor([[-0.5430,  0.7476],\n",
      "        [ 0.3043,  0.9152],\n",
      "        [-0.0288,  0.8600],\n",
      "        [ 0.4537,  0.9093],\n",
      "        [-0.2525,  0.8086],\n",
      "        [-0.9673,  0.6481]])\n",
      "condition:\n",
      "tensor([[-0.9673,  0.6481,  0.4537,  0.9093,  0.3007],\n",
      "        [-0.9673,  0.6481,  0.4537,  0.9093,  0.8968],\n",
      "        [-0.9673,  0.6481,  0.4537,  0.9093,  0.6639],\n",
      "        [-0.9673,  0.6481,  0.4537,  0.9093,  1.0000],\n",
      "        [-0.9673,  0.6481,  0.4537,  0.9093,  0.5055],\n",
      "        [-0.9673,  0.6481,  0.4537,  0.9093,  0.0000]], device='cuda:0')\n",
      "prior_z_mu:\n",
      "tensor([[-0.0467, -0.0009],\n",
      "        [-0.0473, -0.0005],\n",
      "        [-0.0474, -0.0005],\n",
      "        [-0.0470, -0.0008],\n",
      "        [-0.0472, -0.0007],\n",
      "        [-0.0467, -0.0014]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "prior_z_log_sigma_pow2:\n",
      "tensor([[-0.6529, -0.7749],\n",
      "        [-0.6537, -0.7769],\n",
      "        [-0.6533, -0.7759],\n",
      "        [-0.6538, -0.7774],\n",
      "        [-0.6531, -0.7753],\n",
      "        [-0.6528, -0.7757]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "rec_z_mu:\n",
      "tensor([[-0.0469, -0.0011],\n",
      "        [-0.0469, -0.0011],\n",
      "        [-0.0469, -0.0011],\n",
      "        [-0.0469, -0.0011],\n",
      "        [-0.0469, -0.0011],\n",
      "        [-0.0469, -0.0011]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "rec_z_log_sigma_pow2:\n",
      "tensor([[-0.6479, -0.7717],\n",
      "        [-0.6488, -0.7726],\n",
      "        [-0.6482, -0.7721],\n",
      "        [-0.6490, -0.7728],\n",
      "        [-0.6480, -0.7719],\n",
      "        [-0.6478, -0.7717]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "obs_idx = 3\n",
    "path_idx = 150\n",
    "\n",
    "# test to see the error\n",
    "waypoint_dataset, cond_dataset, obs, env_indices \\\n",
    "         = data_loader.load_train_dataset(N=1, NP=1, s=args.s+obs_idx, sp=args.sp+path_idx,\n",
    "                                            data_folder=data_folder, load_dis_ratio=load_dis_ratio)\n",
    "\n",
    "cond_dataset = np.array(cond_dataset)\n",
    "# randomize the dataset before training\n",
    "data=list(zip(waypoint_dataset, cond_dataset, env_indices))\n",
    "random.shuffle(data)\n",
    "waypoint_dataset,cond_dataset,env_indices=list(zip(*data))\n",
    "waypoint_dataset = list(waypoint_dataset)\n",
    "cond_dataset = list(cond_dataset)\n",
    "env_indices = list(env_indices)\n",
    "waypoint_dataset = np.array(waypoint_dataset)\n",
    "cond_dataset = np.array(cond_dataset)\n",
    "env_indices = np.array(env_indices)\n",
    "\n",
    "smpnet.eval()\n",
    "epoch_val_num = 2048\n",
    "waypoint_dataset_i = waypoint_dataset[:epoch_val_num]\n",
    "cond_dataset_i = cond_dataset[:epoch_val_num]\n",
    "env_indices_i = env_indices[:epoch_val_num]\n",
    "bi = waypoint_dataset_i\n",
    "bi = torch.FloatTensor(bi)\n",
    "bi = normalize(bi, args.world_size)\n",
    "bi=to_var(bi)\n",
    "\n",
    "cond_dataset_i = torch.FloatTensor(cond_dataset_i)\n",
    "cond_dataset_i = normalize(cond_dataset_i, args.world_size)  # assume the first Sx2 are start and goal\n",
    "cond_dataset_i = to_var(cond_dataset_i)\n",
    "\n",
    "bobs = obs[env_indices_i].astype(np.float32)\n",
    "bobs = torch.FloatTensor(bobs)\n",
    "bobs = to_var(bobs)\n",
    "loss = smpnet.loss(bi, smpnet.train_forward(bi, cond_dataset_i, bobs), beta=args.beta)\n",
    "loss = torch.mean(loss)\n",
    "print('mean loss: %f' % (loss.cpu().item()))\n",
    "\n",
    "output = smpnet.train_forward(bi, cond_dataset_i, bobs, L=1)\n",
    "gen_loss = smpnet.generation_loss(bi, output)\n",
    "gen_loss = torch.mean(gen_loss)\n",
    "print('generation loss: %f' % (gen_loss.cpu().item()))\n",
    "\n",
    "kl_loss = smpnet.kl_divergence(bi, output)\n",
    "kl_loss = torch.mean(kl_loss)\n",
    "print('KL loss: %f' % (kl_loss.cpu().item()))\n",
    "\n",
    "prior_z_mu, prior_z_log_sigma_pow2, z_mu,z_log_sigma_pow2, z, x_mu = output\n",
    "print('reconstruction: ')\n",
    "print(x_mu)\n",
    "print(x_mu.size())\n",
    "print('ground truth:')\n",
    "print(bi.cpu().data)\n",
    "\n",
    "print('condition:')\n",
    "print(cond_dataset_i[:10])\n",
    "\n",
    "print('prior_z_mu:')\n",
    "print(prior_z_mu)\n",
    "print('prior_z_log_sigma_pow2:')\n",
    "print(prior_z_log_sigma_pow2)\n",
    "\n",
    "print('rec_z_mu:')\n",
    "print(z_mu)\n",
    "print('rec_z_log_sigma_pow2:')\n",
    "print(z_log_sigma_pow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "print('loading...')\n",
    "obs_center, obs, paths, path_lengths = \\\n",
    "        data_loader.load_test_dataset(N=args.no_env, NP=args.no_motion_paths, \\\n",
    "                                      s=args.s, sp=args.sp, folder=data_folder)\n",
    "obs_width = 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "def cvae_visualization(smpnet, obs_idx, path_idx, dist_val, num_sample=10):\n",
    "    # visualize the planning scene and samples\n",
    "    obs_center_i = obs_center[obs_idx]\n",
    "    obs_i = obs[obs_idx]\n",
    "    path_i = paths[obs_idx][path_idx]\n",
    "    path_length_i = path_lengths[obs_idx][path_idx]\n",
    "    #print(path_length_i)\n",
    "    assert path_length_i > 2\n",
    "    #dist = np.linspace(0., 1., 10)\n",
    "    dist = np.array([dist_val])\n",
    "    #print(dist)\n",
    "    dist = np.repeat(dist, num_sample).reshape(-1,1)\n",
    "\n",
    "    #dist = np.repeat(dist, num_sample//10).reshape(-1,1)\n",
    "    #print(dist.shape)\n",
    "    start_i = np.array([path_i[0]])\n",
    "    goal_i = np.array([path_i[path_length_i-1]])\n",
    "    start_i = np.tile(start_i, [num_sample, 1])\n",
    "    goal_i = np.tile(goal_i, [num_sample, 1])\n",
    "    cond_dataset_i = np.concatenate([start_i, goal_i, dist], axis=1)\n",
    "    cond_dataset_i = torch.FloatTensor(cond_dataset_i)\n",
    "    cond_dataset_i = normalize(cond_dataset_i, args.world_size)  # assume the first Sx2 are start and goal\n",
    "    cond_dataset_i = to_var(cond_dataset_i)\n",
    "    \n",
    "    #print([num_sample]+list(obs_i.shape))\n",
    "    bobs = torch.FloatTensor(obs_i).unsqueeze(0).repeat([num_sample, 1, 1, 1])\n",
    "    bobs = to_var(bobs)\n",
    "    #print(bobs.size())\n",
    "    \n",
    "    \n",
    "    # generating\n",
    "    samples = smpnet.gen_forward(cond_dataset_i, obs=bobs, obs_z=None)\n",
    "    samples = samples.cpu().data\n",
    "    samples = unnormalize(samples, args.world_size).numpy()\n",
    "    \n",
    "    # visualize\n",
    "    # show obstacles\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlim(-22, 22)\n",
    "    ax.set_ylim(-22, 22)\n",
    "    #print(obs_center_i.shape)\n",
    "\n",
    "    for i in range(len(obs_center_i)):\n",
    "        x, y = obs_center_i[i][0], obs_center_i[i][1]\n",
    "        obs_patch_i = patches.Rectangle((x-obs_width/2,y-obs_width/2),\\\n",
    "                                       obs_width,obs_width,\\\n",
    "                                       linewidth=0.0, facecolor='black')\n",
    "        ax.add_patch(obs_patch_i)\n",
    "    \n",
    "    # show start and goal\n",
    "    ax.scatter([path_i[0][0]], [path_i[0][1]], c='green', s=100.0)\n",
    "    ax.scatter([path_i[path_length_i-1][0]], [path_i[path_length_i-1][1]], c='red', s=100.0, marker='*')\n",
    "    \n",
    "    # show path\n",
    "    ax.plot(path_i[:path_length_i,0], path_i[:path_length_i,1], c='pink')\n",
    "    ax.scatter(path_i[1:-1,0], path_i[1:-1,1], c=\"pink\", s=100)\n",
    "    # show samples\n",
    "    ax.scatter(samples[:,0], samples[:,1], c='blue', s=50.)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = cvae_visualization(smpnet, 1, 140, dist_val=0.1,num_sample=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  5  9 ...  0  0  0]\n",
      " [14  6  9 ...  0  0  0]\n",
      " [ 2 17  6 ...  0  0  0]\n",
      " ...\n",
      " [14  5  6 ...  0  0  0]\n",
      " [10 17  7 ...  0  0  0]\n",
      " [ 3 21  2 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(path_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smpnet_epoch_1_iter_0.pkl', 'smpnet_epoch_1_iter_2000.pkl', 'smpnet_epoch_1_iter_4000.pkl', 'smpnet_epoch_1_iter_6000.pkl', 'smpnet_epoch_1_iter_8000.pkl', 'smpnet_epoch_1_iter_10000.pkl', 'smpnet_epoch_2_iter_0.pkl', 'smpnet_epoch_2_iter_2000.pkl', 'smpnet_epoch_2_iter_4000.pkl', 'smpnet_epoch_2_iter_6000.pkl', 'smpnet_epoch_2_iter_8000.pkl', 'smpnet_epoch_2_iter_10000.pkl', 'smpnet_epoch_3_iter_0.pkl', 'smpnet_epoch_3_iter_2000.pkl', 'smpnet_epoch_3_iter_4000.pkl', 'smpnet_epoch_3_iter_6000.pkl', 'smpnet_epoch_3_iter_8000.pkl']\n"
     ]
    }
   ],
   "source": [
    "model_fs = [f for f in os.listdir(model_dir) if 'smpnet_epoch_' in f and 'iter' in f]\n",
    "def get_epoch_itr(v_str):\n",
    "    idx = v_str.split('epoch_')[1]\n",
    "    idx = idx.split('_iter_')\n",
    "    epoch = int(idx[0])\n",
    "    itr = int(idx[1][:-4])\n",
    "    return (epoch, itr)\n",
    "model_fs = sorted(model_fs, key=get_epoch_itr)\n",
    "print(model_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_figs(obs_idx, path_idx, num_dist, num_sample=100):\n",
    "    for model_f in model_fs:\n",
    "        if model_f == 'smpnet_epoch_10_iter_1000.pkl':\n",
    "            continue\n",
    "        smpnet = SMPNet(e_net, cvae)\n",
    "        smpnet.eval()\n",
    "        model_path=model_f\n",
    "        load_net_state(smpnet, os.path.join(model_dir, model_path))\n",
    "        torch_seed, np_seed, py_seed = load_seed(os.path.join(model_dir, model_path))\n",
    "        # set seed after loading\n",
    "        torch.manual_seed(torch_seed)\n",
    "        np.random.seed(np_seed)\n",
    "        random.seed(py_seed)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            smpnet.cuda()\n",
    "            smpnet.cvae.cuda()\n",
    "            smpnet.e_net.cuda()\n",
    "        dist_vals = np.linspace(0.,1.,num_dist)\n",
    "        for dist_val in dist_vals:\n",
    "            fig = cvae_visualization(smpnet, obs_idx, path_idx, dist_val, num_sample=num_sample);\n",
    "            os.makedirs('plots/%s/param_%d/' % (args.model_type, args.param_name), exist_ok=True)\n",
    "            plt.savefig(\"plots/%s/param_%d/env_%d_path_%d_dist_%f_%s.png\" % (args.model_type, args.param_name, \\\n",
    "                                                                            obs_idx+args.s, path_idx+args.sp, \\\n",
    "                                                                            dist_val, model_f[:-4]));\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in /ilab/users/ym420/miniconda3/envs/cse535/lib/python3.8/site-packages (2.9.0)\r\n",
      "Requirement already satisfied: pillow in /ilab/users/ym420/miniconda3/envs/cse535/lib/python3.8/site-packages (from imageio) (8.0.0)\r\n",
      "Requirement already satisfied: numpy in /ilab/users/ym420/miniconda3/envs/cse535/lib/python3.8/site-packages (from imageio) (1.19.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "def make_video(obs_idx, path_idx, dist_val):\n",
    "    def tryint(s):\n",
    "        try:\n",
    "            return int(s)\n",
    "        except ValueError:\n",
    "            return s\n",
    "    def str2int(v_str):\n",
    "        idx = v_str.split('epoch_')[1]\n",
    "        idx = int(idx[:-4])\n",
    "        return idx\n",
    "    def get_epoch_itr(v_str):\n",
    "        idx = v_str.split('epoch_')[1]\n",
    "        idx = idx.split('_iter_')\n",
    "        epoch = int(idx[0])\n",
    "        itr = int(idx[1][:-4])\n",
    "        return (epoch, itr)\n",
    "\n",
    "    def sort_humanly(v_list):\n",
    "        return sorted(v_list, key=get_epoch_itr)\n",
    "    image_folder = 'plots/%s/param_%d/' % (args.model_type, args.param_name)\n",
    "    \n",
    "    os.makedirs('video/%s/param_%d' % (args.model_type, args.param_name), exist_ok=True)\n",
    "    video_name = 'video/{}/param_{}/env_{}_path_{}_dist_{}.gif'.format(args.model_type, args.param_name, obs_idx, path_idx, dist_val)\n",
    "    images = [img for img in os.listdir(image_folder) \\\n",
    "              if 'env_%d_path_%d' % (obs_idx+args.s, path_idx+args.sp) in img and 'iter' in img \\\n",
    "                 and 'dist_%f' % (dist_val) in img]\n",
    "    images = sort_humanly(images)\n",
    "    imgs = []\n",
    "    for filename in images:\n",
    "#         print('./'+image_folder+'/'+filename)\n",
    "        imgs.append(imageio.imread('./'+image_folder+'/'+filename))\n",
    "    imageio.mimsave(video_name, imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 1\n",
      "count: 2\n",
      "count: 1\n",
      "count: 2\n",
      "count: 1\n",
      "count: 2\n",
      "count: 1\n",
      "count: 2\n",
      "count: 1\n",
      "count: 2\n",
      "count: 1\n",
      "count: 2\n",
      "count: 1\n",
      "count: 2\n",
      "count: 1\n",
      "count: 2\n",
      "count: 1\n",
      "count: 2\n",
      "count: 1\n",
      "count: 2\n"
     ]
    }
   ],
   "source": [
    "# save figure for different model\n",
    "for obs_idx in range(10):\n",
    "    path_idxs = []\n",
    "    count = 0\n",
    "    while True:\n",
    "        path_idx = np.random.choice(200)\n",
    "        if path_lengths[obs_idx][path_idx] < 5:\n",
    "            continue\n",
    "        if path_idx in path_idxs:\n",
    "            continue\n",
    "        count += 1\n",
    "        path_idxs.append(path_idx)\n",
    "        num_dist = 11\n",
    "        gen_figs(obs_idx, path_idx, num_dist=num_dist, num_sample=20)\n",
    "        dist_vals = np.linspace(0.,1.,num_dist)\n",
    "        for dist_val in dist_vals:\n",
    "            make_video(obs_idx, path_idx, dist_val)\n",
    "        print('count: %d' % (count))\n",
    "        if count == 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_video(obs_idx+args.s, path_idx+args.sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cse535] *",
   "language": "python",
   "name": "conda-env-cse535-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
